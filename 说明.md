# ownerscraper
股票持仓情况爬虫，爬取300多公司的数据并存为 json 文件。

## [ ExtractRule ] 撰写规则:
函数组合可以直接写在 excel 里面

函数组合的列子:
> {"m": "fetch", "u": 1, "t": [0], "f": [{"del_rows_end": 1}], "k": ["Investor", "Amount", "%"]}

> {"m": "fetch", "u": "span", "t": [0], "pre": [{"replace_text": ["100%", "100"]}], "k": [1, 2, 3]}

插件名称写在 excel 里面， 然后程序会使用 jobs 文件夹里的插件

插件组合的列子:
> {"p": "FI4000292438_anora_com"}

#### m:
获取网页源码的方法，有：fetch, chrome, fetch_url, chrome_url

#### u:
提取更新日期的参数,可以使用 css 选择器， xpath， 带负号 - 的规则可以提取 28.09.2021 形式的日期

- 0: 日期为空
- 1: 日期为 (updated|Sist oppdatert) + 数字日期形式
- 2: 日期为英文月份形式 - 21 March 2018
- 9: 直接使用传入日期 
- "MONTH_START": 每月第一天
- "//p[contains(text(), 'updated ')]" xpath 方式
- "p.updated" CSS 选择器方式

#### t:
有些页面可能存在多个表格，0为页面里第一个表格

#### f:
提取 table 后会使用的多个函数，具体查看 fc.py 例如 [{"del_rows_end": 1}, {"del_rows_front": 1}]

#### pre:
提取 table 前会使用的多个函数，具体如上

#### k:
按照 config.py 里的 TABLE_MAIN_KEYS 顺序来命名主要的 key，如果提取的表格 key 为数字，则参数为数字，否则为字符串
列子: 
> ["Investor", "Amount", "%"]

> [1, 2, 3]

> [2, 1, 3]

> [1, "", 3]
